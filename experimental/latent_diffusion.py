# %%

import sys, pathlib
sys.path.append(str(pathlib.Path(__file__).parent.parent))

from typing import List, Tuple, Sequence, Collection
from IPython.display import clear_output
from PIL import Image
import matplotlib.pyplot as plt
import pprint
import warnings
import numpy as np

import jax
import jax.numpy as jnp
import flax.linen as nn

import viton
import viton.nn.ninjax as nj
from viton.nn import nets
from viton.nn.nets import *
from viton.nn import jaxutils
from viton.nn.jaxutils import gseed
from viton.nn.unet import AttentionUnet

warnings.filterwarnings('ignore', '.*input data to the valid range.*')

transform = lambda x: x / 255.0 * 2 - 1
untransform = lambda x: ((x / 2 + 0.5) * 255.0).astype(np.uint8)


class CrossAttention(nj.Module):
  """Implement multi-head attention based on:
      https://arxiv.org/pdf/1706.03762.pdf
      https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html
      https://www.kaggle.com/code/darshan1504/exploring-diffusion-models-with-jax

    An attention function can be described as mapping a query and a set of key-value
    pairs to an output, where the query, keys, values, and output are all vectors.
    The output is computed as a weighted sum of the values, where the weight assigned
    to each value is computed by a compatibility function of the query with the
    corresponding key.

    NOTE: this implement the simplified version of dimension, in the actual paper,
      the key dimension and value dimension are different from each other
  """
  def __init__(self, dim: int, heads: int, **kwargs):
    assert dim % heads == 0, "dim must be divisible by heads"
    self._dim = dim
    self._heads = heads
    self._kwargs = {**kwargs, 'act': 'none'} # make sure act is none

  def __call__(self, inputs: jax.Array, conditions: jax.Array) -> jax.Array:
    """By default for cross attention, we have the following schema:
        inputs (jax.Array): (B, T_target, D_query): input is the query
        conditions (jax.Array): (B, T_source, D_value): condition is used to generate key and value
      where T is the time axis and D is the number of dimensions.
      The time axis is where we want to compute the attention map
      toward.
      =>  K (B, T_source, D_query): the value and the key are generated by the source timeline
          V (B, T_source, D_value): the value and the key are generated by the source timeline
          Q (B, T_target, D_query): the query, the flow we are actually follow, the target time/attention axis
        => Q@K^T (B, T_target, T_source)
        => (Q@K^T)@V (B, T_target, D_value)
    Args:
      inputs (jax.Array): (B, H, W, C).
        the height and width dimension will be flatten to achieve the shape
        (B, T, D)
      conditions (jax.Array): (B, L, C). Some condition represented in time.
        The time here can be 1 for a single condition

    Returns:
      jax.Array: the attended output
    """
    B, *T, C = inputs.shape
    B, L, Z = conditions.shape
    pT = np.prod(T) # product of T
    _inputs = inputs.reshape((B, pT, C)) # (B, pT, C)
    scale = 1.0 / jnp.sqrt(self._dim // self._heads) # ()
    kv: jax.Array = self.get("kv", Linear, 2 * self._dim, **self._kwargs)(conditions) # (B, L, Z) => (B, L, 2D)
    kv = kv.reshape((B, L, 2, self._heads, self._dim // self._heads)) # (B, L, 3, H, 3D)
    key, value = kv[:, :, 0], kv[:, :, 1] # (B, L, H, D)
    q: jax.Array = self.get("q", Linear, self._dim, **self._kwargs)(_inputs) # (B, pT, C) => (B, pT, D)
    query = q.reshape((B, pT, self._heads, self._dim // self._heads)) # (B, pT, H, 3D)
    score = jnp.einsum("bqhd,bkhd->bqhk", query, key) # Q @ K^T => (B, pTq, H, pTk)
    norm_score = score * scale # (B, pTq, H, pTk)
    attention_weights = jax.nn.softmax(norm_score, -1) # (B, pTq, H, pTk)
    attention_map = jnp.einsum("bqhk,bkhd->bqhd", attention_weights, value) # (B, pTq, H, pTk) @ (B, pTk, H, D) => (B, pTq, H, D)
    attention_map = attention_map.reshape((B, *T, -1)) # (B, *T, D)
    out = self.get("out", Linear, self._dim, **self._kwargs)(attention_map) # (B, T_out, L_query)
    return out

model = CrossAttention(4, 2, name="ca")
B, H, W, C, T_cond, Z_cond = 1, 4, 4, 8, 1, 6
inputs = jnp.asarray(np.random.normal(0, 1, (B, H, W, C)))
cond = jnp.asarray(np.random.normal(0, 1, (B, T_cond, Z_cond)))
params = nj.init(model)({}, inputs, cond, seed=gseed())
pprint.pprint(jax.tree.map(lambda x: x.shape, params))
forward = jax.jit(nj.pure(model))
params, out = forward(params, inputs, cond)
print(out)

# %%


class LatentUnetCrossAttention(nj.Module):
  def __init__(self, dims: List[int]=[16, 32, 128, 512], heads=4, time_embedding=256) -> None:
    self._dims = dims
    self._heads = heads
    self._time_embedding = time_embedding

  def __call__(self, inputs: jax.Array, time_index: jax.Array, condition: jax.Array) -> jax.Array:
    """

    Args:
      inputs (jax.Array): (B, Z1)
      time_index (jax.Array): (B, 1)
      condition (jax.Array): (B, Z2)

    Returns:
      jax.Array: (B, Z1)
    """
    B, H, W, C = inputs.shape
    x = self.get("conv", nets.Conv2D, self._dims[0], 1)(inputs)
    time_embed = self.get("time", nets.TimeEmbedding, self._time_embedding)(time_index)

    carries = []

    #### downsampling
    for stage, dim in enumerate(self._dims):
      # two res blocks + self attention + add together
      x = self.get(f"dr{stage}0", nets.ResnetBlock, dim)(x, time_embed)
      x = self.get(f"dr{stage}1", nets.ResnetBlock, dim)(x, time_embed)
      norm = self.get(f"dn{stage}", nets.Norm, "layer")(x)
      att = self.get(f"da{stage}", nets.SelfAttention, dim, self._heads)(norm)
      # att = self.get(f"da{stage}", nj.FromFlax(nn.MultiHeadDotProductAttention), self._heads, qkv_features=dim)(norm)
      x = x + att
      carries.append(x)
      # print(x.shape)
      # Downsampling
      if stage != len(self._dims) - 1:
        x = self.get(f"dd{stage}", nets.Conv2D, dim, 3, stride=2)(x)

    #### bottleneck
    x = self.get(f"br0", nets.ResnetBlock, self._dims[-1])(x, time_embed)
    norm = self.get(f"bn", nets.Norm, "layer")(x)
    att = self.get(f"ba", nets.SelfAttention, self._dims[-1], self._heads)(norm)
    # att = self.get(f"ba", nj.FromFlax(nn.MultiHeadDotProductAttention), self._heads, qkv_features=dim)(norm)
    x = norm + x
    x = self.get(f"br1", nets.ResnetBlock, self._dims[-1])(x, time_embed)

    #### upsampling
    # Upsampling phase
    for stage, dim in enumerate(reversed(self._dims)):
      x = jnp.concatenate([carries.pop(), x], -1)
      # two res blocks + self attention + add together
      x = self.get(f"ur{stage}0", nets.ResnetBlock, dim)(x, time_embed)
      x = self.get(f"ur{stage}1", nets.ResnetBlock, dim)(x, time_embed)
      norm = self.get(f"un{stage}", nets.Norm, "layer")(x)
      att = self.get(f"ua{stage}", nets.SelfAttention, dim, self._heads)(norm)
      # att = self.get(f"ua{stage}", nj.FromFlax(nn.MultiHeadDotProductAttention), self._heads, qkv_features=dim)(norm)
      x = x + att
      # Upsampling
      if stage != len(self._dims) - 1:
        x = self.get(f"uu{stage}", nets.Conv2D, dim, 3, stride=2, transp=True)(x)

    # Final ResNet block and output convolutional layer
    x = self.get(f"fr", nets.ResnetBlock, self._dims[0])(x, time_embed)
    x = self.get(f"out", nets.Conv2D, C, 1, stride=1)(x)
    return x

class LatentDiffuser(nj.Module):
  # implement the algorithm from https://arxiv.org/pdf/2006.11239.pdf
  # adapt from: https://github.com/andylolu2/jax-diffusion/blob/main/jax_diffusion/diffusion.py
  def __init__(self, beta_start: float, beta_final: float,
      steps: int, time_embedding: int=128, dims: int=[16, 16, 16, 16], heads: int=4):
    """_summary_

    Args:
      beta_start (float): the beta/variance of x_0 or the observation/frame/image
      beta_final (float): the beta/variance of x_T-1 or the latent/diffused noise
      steps (int): the total number of steps in the mdp
    """
    self._betas = np.linspace(beta_start, beta_final, steps) # (T,)
    self._alphas = 1 - self._betas # (T,)
    self._alpha_bars = np.cumprod(self._alphas) # (T,)
    self._steps = steps # ()
    self.unet = AttentionLatentUnet(dims, heads, time_embedding, name="unet")

  def forward(self, x_0: jax.Array, t: jax.Array) -> jax.Array:
    # given the image, add noise to it. See algorithm 1 in https://arxiv.org/pdf/2006.11239.pdf
    # For forward, we can compute a random t
    B, H, W, C = x_0.shape
    B, I = t.shape
    assert I == 1
    """x_t, eps = self.sample_q(x_0, t): Samples x_t given x_0 by the q(x_t|x_0) formula."""
    # x_0: (B, H, W, C)
    alpha_bar_t = self._alpha_bars.take(t.astype(jnp.int32)) # (B, 1)
    alpha_bar_t = alpha_bar_t[: None, None, :] # (B, 1, 1, 1)
    eps = jax.random.normal(nj.seed(), shape=x_0.shape, dtype=x_0.dtype)
    x_t = jnp.sqrt(alpha_bar_t) * x_0 + jnp.sqrt(1 - alpha_bar_t) * eps
    """end of x_t, eps = self.sample_q(x_0, t)"""
    return x_t.clip(-1, 1), eps

  def reverse_step(self, x_t: jax.Array, t: jax.Array):
    """See algorithm 2 in https://arxiv.org/pdf/2006.11239.pdf"""
    B, H, W, C = x_t.shape
    B, I = t.shape
    assert I == 1
    alpha_t = jnp.take(self._alphas, t)
    alpha_bar_t = jnp.take(self._alpha_bars, t)
    sigma_t = jnp.sqrt(jnp.take(self._betas, t))
    z = (t > 0) * jax.random.normal(nj.seed(), shape=x_t.shape, dtype=x_t.dtype)
    eps = self.unet(x_t, t)
    x = (1.0 / jnp.sqrt(alpha_t)) * (
      x_t - ((1 - alpha_t) / jnp.sqrt(1 - alpha_bar_t)) * eps
    ) + sigma_t * z
    x = x.clip(-1, 1)
    return x, x

  def reverse(self, x_T: jax.Array) -> jax.Array:
    B, H, W, C = x_T.shape
    # given the noise, reconstruct the image
    # For reverse, we have to reverse it one by one
    ts = jnp.arange(0, self._steps)[..., None, None] # (T, 1, 1)
    ts = jnp.repeat(ts, B, axis=1) # (T, B, 1)
    x_hat_0, xs = nj.scan(self.reverse_step, x_T, ts, reverse=True, unroll=1, axis=0)
    return x_hat_0, xs





# %%


class Agent(nj.Module):
  def __init__(self, config):
    self.config = config
    self.diffuser = Diffuser(**config.diffuser, name="diff")
    self.opt = jaxutils.Optimizer(**config.opt, name="opt")

  def train(self, data):
    modules = [self.diffuser]
    opt_metrics, (outs, states, loss_metrics) = self.opt(modules,
      self.loss, data, has_aux=True)
    opt_metrics.update(loss_metrics)
    return outs, states, opt_metrics

  def infer(self, data):
    return self.diffuser.reverse(data["image"])

  def loss(self, data):
    x_0 = data["image"]
    B, H, W, C = x_0.shape

    # Generate random timesteps indices
    timesteps = np.random.randint(0, self.diffuser._steps, (B, 1))
    timesteps = jaxutils.cast_to_compute(timesteps)

    # Generating the noise and noisy image for this batch
    # Add noise to x_0 until timestep
    noisy_image, noise = self.diffuser.forward(x_0, timesteps)

    # Forward noising: given a noisy image, predict the noise added to that image
    pred_noise = self.diffuser.unet(noisy_image, timesteps)

    # l1 loss
    # loss = ((pred_noise - noise)**2).mean([-3, -2, -1]).mean()
    loss = ((pred_noise - noise)**2).mean()

    # metrics
    outs = {"pred_noise": pred_noise, "noise": noise, "noisy_image": noisy_image}
    states = {}
    metrics = {"loss": loss}
    return loss, (outs, states, metrics)


config = viton.core.config.Config(
  diffuser=dict(
    beta_start=0.0003, beta_final=0.01,
    steps=200, time_embedding=128, dims=[64, 128, 256, 512], heads=8
  ),
  opt=dict(
    lr=1e-4, opt='adam', eps=0.00001, clip=100, warmup = 0,
    wd= 0, wd_pattern = r'/(w|kernel)$', lateclip = 0
  )
)
A = Agent(config, name="agent")
img = Image.open("aux/elefant.jpg")
img = img.resize((64, 64))
img = transform(np.asarray(img))
img = jnp.asarray(img)[None]
data = {"image": img}

params = nj.init(A.train)({}, data, seed=gseed())

# Purifying
train = jax.jit(nj.pure(A.train))
infer = jax.jit(nj.pure(A.infer))


